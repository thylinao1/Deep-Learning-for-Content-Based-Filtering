{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Collaborative Filtering: Two-Tower Architecture\n",
    "\n",
    "**Objective:** Build a deep learning recommender system that learns latent representations of users and movies in a shared embedding space.\n",
    "\n",
    "**Key Innovation:** Rather than manually engineering features or using simple matrix factorization, we use neural networks to automatically discover complex, nonlinear patterns in user preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dot\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tabulate\n",
    "from recsysNN_utils import *\n",
    "\n",
    "# Display settings\n",
    "pd.set_option(\"display.precision\", 1)\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration\n",
    "\n",
    "The dataset contains user-movie interactions with ratings. We'll examine both:\n",
    "1. **Content-based baselines** (for comparison)\n",
    "2. **Raw training data** (for neural collaborative filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content-based baseline recommendations (for comparison)\n",
    "top10_df = pd.read_csv(\"./data/content_top10_df.csv\")\n",
    "bygenre_df = pd.read_csv(\"./data/content_bygenre_df.csv\")\n",
    "\n",
    "print(\"Top 10 Content-Based Recommendations:\")\n",
    "display(top10_df)\n",
    "\n",
    "print(\"\\nGenre-Based Recommendations:\")\n",
    "display(bygenre_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Structure\n",
    "\n",
    "Load the collaborative filtering dataset with user and item features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "item_train, user_train, y_train, item_features, user_features, \\\n",
    "    item_vecs, movie_dict, user_to_genre = load_data()\n",
    "\n",
    "# Feature dimensions\n",
    "num_user_features = user_train.shape[1] - 3  # Exclude user_id, rating_count, avg_rating\n",
    "num_item_features = item_train.shape[1] - 1  # Exclude movie_id\n",
    "\n",
    "# Column slicing indices for training\n",
    "uvs = 3  # User genre vector start\n",
    "ivs = 3  # Item genre vector start\n",
    "u_s = 3  # Start of user features to use\n",
    "i_s = 1  # Start of item features to use\n",
    "\n",
    "print(f\"User feature dimension: {num_user_features}\")\n",
    "print(f\"Item feature dimension: {num_item_features}\")\n",
    "print(f\"Training samples: {len(y_train):,}\")\n",
    "print(f\"Unique movies: {len(item_vecs):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preview: User Features\n",
    "\n",
    "**User features include:**\n",
    "- User ID\n",
    "- Average rating given\n",
    "- Number of ratings\n",
    "- Genre preference vector (14 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint_train(user_train, user_features, uvs, u_s, maxcount=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preview: Item Features\n",
    "\n",
    "**Movie features include:**\n",
    "- Movie ID\n",
    "- Release year\n",
    "- Genre flags (14 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint_train(item_train, item_features, ivs, i_s, maxcount=5, user=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable: Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sample ratings: {y_train[:10]}\")\n",
    "print(f\"Rating range: [{y_train.min()}, {y_train.max()}]\")\n",
    "print(f\"Mean rating: {y_train.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "**Why scale?**\n",
    "\n",
    "1. **Convergence speed:** Features on different scales slow gradient descent\n",
    "2. **Numerical stability:** Prevents overflow/underflow in activations\n",
    "3. **Weight initialization:** Assumes inputs are zero-mean, unit-variance\n",
    "\n",
    "**Scaling strategies:**\n",
    "- **StandardScaler:** For user/item features → $z = \\frac{x - \\mu}{\\sigma}$\n",
    "- **MinMaxScaler(-1, 1):** For ratings → enables tanh-like output interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve unscaled versions for display\n",
    "item_train_unscaled = item_train.copy()\n",
    "user_train_unscaled = user_train.copy()\n",
    "y_train_unscaled = y_train.copy()\n",
    "\n",
    "# Standardize features (zero mean, unit variance)\n",
    "scalerItem = StandardScaler()\n",
    "item_train = scalerItem.fit_transform(item_train)\n",
    "\n",
    "scalerUser = StandardScaler()\n",
    "user_train = scalerUser.fit_transform(user_train)\n",
    "\n",
    "# Scale ratings to [-1, 1] for smoother optimization\n",
    "scalerTarget = MinMaxScaler((-1, 1))\n",
    "y_train = scalerTarget.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(\"Scaling complete.\")\n",
    "print(f\"User features: mean={user_train.mean():.3f}, std={user_train.std():.3f}\")\n",
    "print(f\"Item features: mean={item_train.mean():.3f}, std={item_train.std():.3f}\")\n",
    "print(f\"Target ratings: range=[{y_train.min():.2f}, {y_train.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "80/20 split with fixed random state for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_train, item_test = train_test_split(item_train, train_size=0.80, shuffle=True, random_state=1)\n",
    "user_train, user_test = train_test_split(user_train, train_size=0.80, shuffle=True, random_state=1)\n",
    "y_train, y_test = train_test_split(y_train, train_size=0.80, shuffle=True, random_state=1)\n",
    "\n",
    "print(f\"Training samples: {len(y_train):,}\")\n",
    "print(f\"Test samples: {len(y_test):,}\")\n",
    "print(f\"Train/test ratio: {len(y_train)/len(y_test):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Tower Architecture\n",
    "\n",
    "### Design Philosophy\n",
    "\n",
    "**Why two separate towers?**\n",
    "\n",
    "1. **Decoupled learning:** User and item representations evolve independently\n",
    "2. **Scalability:** Precompute item embeddings once, score millions of users in real-time\n",
    "3. **Interpretability:** Can analyze user/item clusters separately\n",
    "\n",
    "**Architecture choice: 256 → 128 → 32**\n",
    "\n",
    "- **Layer 1 (256):** High-capacity feature extraction\n",
    "- **Layer 2 (128):** Dimensionality reduction + abstraction\n",
    "- **Layer 3 (32):** Compact embedding for dot-product similarity\n",
    "\n",
    "**Why ReLU?** Non-saturating, computationally efficient, empirically strong\n",
    "\n",
    "**Why linear output?** Embeddings are constrained by L2 normalization, not activation\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "**User Tower:**\n",
    "$$\n",
    "\\mathbf{v}_u = \\text{L2Normalize}\\left( \\phi_u(\\mathbf{x}_u; \\theta_u) \\right)\n",
    "$$\n",
    "\n",
    "**Item Tower:**\n",
    "$$\n",
    "\\mathbf{v}_m = \\text{L2Normalize}\\left( \\phi_m(\\mathbf{x}_m; \\theta_m) \\right)\n",
    "$$\n",
    "\n",
    "**Predicted Rating:**\n",
    "$$\n",
    "\\hat{r}_{u,m} = \\mathbf{v}_u^T \\mathbf{v}_m = \\sum_{k=1}^{32} v_{u,k} \\cdot v_{m,k}\n",
    "$$\n",
    "\n",
    "**Loss (MSE):**\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( r_i - \\hat{r}_i \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding dimension\n",
    "embedding_dim = 32\n",
    "\n",
    "# User Tower: Maps user features → 32D embedding\n",
    "user_NN = Sequential([\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(embedding_dim, activation='linear')\n",
    "], name='user_tower')\n",
    "\n",
    "# Item Tower: Maps movie features → 32D embedding\n",
    "item_NN = Sequential([\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(embedding_dim, activation='linear')\n",
    "], name='item_tower')\n",
    "\n",
    "# Build full model with Keras Functional API\n",
    "input_user = Input(shape=(num_user_features,), name='user_input')\n",
    "vu = user_NN(input_user)\n",
    "vu = tf.linalg.l2_normalize(vu, axis=1)  # Normalize to unit sphere\n",
    "\n",
    "input_item = Input(shape=(num_item_features,), name='item_input')\n",
    "vm = item_NN(input_item)\n",
    "vm = tf.linalg.l2_normalize(vm, axis=1)  # Normalize to unit sphere\n",
    "\n",
    "# Dot product: measures cosine similarity (since vectors are normalized)\n",
    "output = Dot(axes=1, name='dot_product')([vu, vm])\n",
    "\n",
    "# Final model\n",
    "model = keras.Model([input_user, input_item], output, name='two_tower_recsys')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why L2 Normalization?\n",
    "\n",
    "Constraining embeddings to the unit hypersphere ($||\\mathbf{v}||_2 = 1$) has several benefits:\n",
    "\n",
    "1. **Dot product = Cosine similarity:**\n",
    "   $$\\mathbf{v}_u^T \\mathbf{v}_m = ||\\mathbf{v}_u|| \\cdot ||\\mathbf{v}_m|| \\cdot \\cos(\\theta) = \\cos(\\theta)$$\n",
    "\n",
    "2. **Scale invariance:** Prevents magnitude from dominating similarity\n",
    "\n",
    "3. **Gradient stability:** Bounded outputs prevent exploding gradients\n",
    "\n",
    "4. **Geometric interpretation:** Embeddings live on a 32-dimensional sphere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compilation and Training\n",
    "\n",
    "**Optimizer:** Adam with learning rate 0.01\n",
    "- Adaptive learning rates per parameter\n",
    "- Momentum for faster convergence\n",
    "- Empirically robust across architectures\n",
    "\n",
    "**Loss:** Mean Squared Error (regression task)\n",
    "\n",
    "**Training strategy:**\n",
    "- Batch size: 256 (balance between gradient noise and memory)\n",
    "- Epochs: 30 (sufficient for convergence on this dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.01),\n",
    "    loss=MeanSquaredError()\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    [user_train[:, u_s:], item_train[:, i_s:]],\n",
    "    y_train,\n",
    "    epochs=30,\n",
    "    batch_size=256,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.log10(history.history['loss']), label='log10(Training Loss)')\n",
    "plt.plot(np.log10(history.history['val_loss']), label='log10(Validation Loss)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('log10(MSE)')\n",
    "plt.title('Log-Scale Learning Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final validation loss: {history.history['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss = model.evaluate(\n",
    "    [user_test[:, u_s:], item_test[:, i_s:]],\n",
    "    y_test,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"Test MSE: {test_loss:.4f}\")\n",
    "print(f\"Test RMSE: {np.sqrt(test_loss):.4f}\")\n",
    "\n",
    "# Convert back to original rating scale\n",
    "test_rmse_original = np.sqrt(test_loss) * (y_train_unscaled.max() - y_train_unscaled.min())\n",
    "print(f\"Test RMSE (original scale): {test_rmse_original:.3f} stars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: Recommend Movies for New User\n",
    "\n",
    "**Workflow:**\n",
    "1. User specifies genre preferences (0-5 scale)\n",
    "2. Create user feature vector\n",
    "3. Replicate across all movies\n",
    "4. Compute predictions (dot products in embedding space)\n",
    "5. Sort and return top-N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new user profile\n",
    "new_user_preferences = {\n",
    "    'Action': 5,\n",
    "    'Adventure': 5,\n",
    "    'Animation': 0,\n",
    "    'Children': 0,\n",
    "    'Comedy': 0,\n",
    "    'Crime': 3,\n",
    "    'Documentary': 0,\n",
    "    'Drama': 0,\n",
    "    'Fantasy': 5,\n",
    "    'Horror': 0,\n",
    "    'Mystery': 4,\n",
    "    'Romance': 0,\n",
    "    'SciFi': 5,\n",
    "    'Thriller': 4\n",
    "}\n",
    "\n",
    "print(\"New user profile:\")\n",
    "for genre, rating in new_user_preferences.items():\n",
    "    if rating > 0:\n",
    "        print(f\"  {genre}: {rating}/5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct user feature vector\n",
    "# Format: [user_id, avg_rating, rating_count, genre_preferences...]\n",
    "new_user_id = 0\n",
    "new_avg_rating = 0\n",
    "new_rating_count = 0\n",
    "\n",
    "user_vec = np.array([\n",
    "    [new_user_id, new_avg_rating, new_rating_count,\n",
    "     new_user_preferences['Action'],\n",
    "     new_user_preferences['Adventure'],\n",
    "     new_user_preferences['Animation'],\n",
    "     new_user_preferences['Children'],\n",
    "     new_user_preferences['Comedy'],\n",
    "     new_user_preferences['Crime'],\n",
    "     new_user_preferences['Documentary'],\n",
    "     new_user_preferences['Drama'],\n",
    "     new_user_preferences['Fantasy'],\n",
    "     new_user_preferences['Horror'],\n",
    "     new_user_preferences['Mystery'],\n",
    "     new_user_preferences['Romance'],\n",
    "     new_user_preferences['SciFi'],\n",
    "     new_user_preferences['Thriller']]\n",
    "])\n",
    "\n",
    "# Replicate user vector for all movies\n",
    "user_vecs = gen_user_vecs(user_vec, len(item_vecs))\n",
    "\n",
    "# Scale features\n",
    "suser_vecs = scalerUser.transform(user_vecs)\n",
    "sitem_vecs = scalerItem.transform(item_vecs)\n",
    "\n",
    "# Predict ratings\n",
    "y_pred = model.predict([suser_vecs[:, u_s:], sitem_vecs[:, i_s:]], verbose=0)\n",
    "\n",
    "# Unscale predictions\n",
    "y_pred_unscaled = scalerTarget.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Sort by predicted rating (descending)\n",
    "sorted_indices = np.argsort(-y_pred_unscaled)\n",
    "sorted_predictions = y_pred_unscaled[sorted_indices]\n",
    "sorted_items = item_vecs[sorted_indices]\n",
    "\n",
    "print(\"\\n🎬 Top 10 Recommended Movies:\\n\")\n",
    "print_pred_movies(sorted_predictions, sorted_items, movie_dict, maxcount=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: Predict for Existing User\n",
    "\n",
    "Compare model predictions against actual ratings for a user in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select user\n",
    "uid = 2\n",
    "\n",
    "# Generate (user, item) pairs for all movies\n",
    "user_vecs, y_actual = get_user_vecs(uid, user_train_unscaled, item_vecs, user_to_genre)\n",
    "\n",
    "# Scale and predict\n",
    "suser_vecs = scalerUser.transform(user_vecs)\n",
    "sitem_vecs = scalerItem.transform(item_vecs)\n",
    "y_pred = model.predict([suser_vecs[:, u_s:], sitem_vecs[:, i_s:]], verbose=0)\n",
    "y_pred_unscaled = scalerTarget.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Sort by prediction\n",
    "sorted_indices = np.argsort(-y_pred_unscaled)\n",
    "sorted_predictions = y_pred_unscaled[sorted_indices]\n",
    "sorted_items = item_vecs[sorted_indices]\n",
    "sorted_user = user_vecs[sorted_indices]\n",
    "sorted_actual = y_actual[sorted_indices]\n",
    "\n",
    "print(f\"\\n📊 Predictions vs. Actual Ratings for User {uid}:\\n\")\n",
    "print_existing_user(\n",
    "    sorted_predictions, \n",
    "    sorted_actual.reshape(-1, 1), \n",
    "    sorted_user, \n",
    "    sorted_items, \n",
    "    ivs, \n",
    "    uvs, \n",
    "    movie_dict, \n",
    "    maxcount=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Analysis: Movie Similarity\n",
    "\n",
    "### Extract Item Embeddings\n",
    "\n",
    "We can isolate the item tower to generate embeddings for all movies. These embeddings capture latent factors that explain user preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_squared(a, b):\n",
    "    \"\"\"\n",
    "    Compute squared Euclidean distance between two vectors.\n",
    "    \n",
    "    d² = Σ(aᵢ - bᵢ)²\n",
    "    \n",
    "    Args:\n",
    "        a, b: numpy arrays of shape (n,)\n",
    "    \n",
    "    Returns:\n",
    "        float: squared distance\n",
    "    \"\"\"\n",
    "    return np.sum((a - b) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick validation\n",
    "a1 = np.array([1.0, 2.0, 3.0])\n",
    "b1 = np.array([1.0, 2.0, 3.0])\n",
    "print(f\"Distance (identical vectors): {euclidean_distance_squared(a1, b1):.3f} (expect 0)\")\n",
    "\n",
    "a2 = np.array([1.1, 2.1, 3.1])\n",
    "b2 = np.array([1.0, 2.0, 3.0])\n",
    "print(f\"Distance (small difference): {euclidean_distance_squared(a2, b2):.3f} (expect 0.03)\")\n",
    "\n",
    "a3 = np.array([0, 1, 0])\n",
    "b3 = np.array([1, 0, 0])\n",
    "print(f\"Distance (orthogonal): {euclidean_distance_squared(a3, b3):.3f} (expect 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Item-Only Model\n",
    "\n",
    "Extract the item tower with L2 normalization to generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model that outputs only item embeddings\n",
    "input_item_standalone = Input(shape=(num_item_features,))\n",
    "vm_standalone = item_NN(input_item_standalone)\n",
    "vm_standalone = tf.linalg.l2_normalize(vm_standalone, axis=1)\n",
    "\n",
    "item_model = keras.Model(input_item_standalone, vm_standalone, name='item_embeddings')\n",
    "item_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate All Movie Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and embed all movies\n",
    "scaled_item_vecs = scalerItem.transform(item_vecs)\n",
    "movie_embeddings = item_model.predict(scaled_item_vecs[:, i_s:], verbose=0)\n",
    "\n",
    "print(f\"Movie embedding matrix shape: {movie_embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {movie_embeddings.shape[1]}\")\n",
    "print(f\"Number of movies: {movie_embeddings.shape[0]}\")\n",
    "\n",
    "# Verify L2 normalization\n",
    "norms = np.linalg.norm(movie_embeddings, axis=1)\n",
    "print(f\"\\nEmbedding norms (should all be ~1.0):\")\n",
    "print(f\"  Mean: {norms.mean():.6f}\")\n",
    "print(f\"  Std:  {norms.std():.6f}\")\n",
    "print(f\"  Min:  {norms.min():.6f}\")\n",
    "print(f\"  Max:  {norms.max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Similar Movies\n",
    "\n",
    "**Method:** Compute pairwise distances in embedding space\n",
    "\n",
    "**Interpretation:** Movies with small embedding distance share latent characteristics that drive user preferences, even if their explicit features (genres) differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise distance matrix\n",
    "n_movies = len(movie_embeddings)\n",
    "distance_matrix = np.zeros((n_movies, n_movies))\n",
    "\n",
    "for i in range(n_movies):\n",
    "    for j in range(n_movies):\n",
    "        distance_matrix[i, j] = euclidean_distance_squared(\n",
    "            movie_embeddings[i], \n",
    "            movie_embeddings[j]\n",
    "        )\n",
    "\n",
    "# Mask diagonal (self-distance)\n",
    "masked_distances = ma.masked_array(distance_matrix, mask=np.eye(n_movies))\n",
    "\n",
    "print(\"Distance matrix computed.\")\n",
    "print(f\"Shape: {distance_matrix.shape}\")\n",
    "print(f\"\\nSample distances for movie 0:\")\n",
    "print(f\"  Min: {masked_distances[0].min():.4f}\")\n",
    "print(f\"  Mean: {masked_distances[0].mean():.4f}\")\n",
    "print(f\"  Max: {masked_distances[0].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbor Table\n",
    "\n",
    "For each movie, find its nearest neighbor in embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build similarity table\n",
    "count = 50  # Number of movies to display\n",
    "similarity_table = [[\"Movie\", \"Genres\", \"Most Similar Movie\", \"Genres\"]]\n",
    "\n",
    "for i in range(min(count, n_movies)):\n",
    "    # Find nearest neighbor\n",
    "    nearest_idx = np.argmin(masked_distances[i])\n",
    "    \n",
    "    # Get movie details\n",
    "    movie1_id = int(item_vecs[i, 0])\n",
    "    movie2_id = int(item_vecs[nearest_idx, 0])\n",
    "    \n",
    "    similarity_table.append([\n",
    "        movie_dict[movie1_id]['title'],\n",
    "        movie_dict[movie1_id]['genres'],\n",
    "        movie_dict[movie2_id]['title'],\n",
    "        movie_dict[movie2_id]['genres']\n",
    "    ])\n",
    "\n",
    "# Display as HTML table\n",
    "from IPython.display import HTML\n",
    "html_table = tabulate.tabulate(similarity_table, tablefmt='html', headers='firstrow')\n",
    "display(HTML(html_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "### What the Model Learned\n",
    "\n",
    "1. **Latent Taste Profiles:** The 32-dimensional embedding space captures abstract preferences beyond genre labels\n",
    "\n",
    "2. **Automatic Feature Discovery:** The network learns which combinations of genres/attributes matter for prediction\n",
    "\n",
    "3. **Similarity Structure:** Movies cluster by shared audience appeal, not just explicit features\n",
    "\n",
    "### Why This Beats Simple Approaches\n",
    "\n",
    "| Method | Representation | Limitation |\n",
    "|--------|---------------|------------|\n",
    "| **Content-Based** | Boolean genre flags | Ignores user-specific preferences |\n",
    "| **Matrix Factorization** | Linear embeddings | Cannot model nonlinear interactions |\n",
    "| **Two-Tower DNN** | Learned embeddings | ✓ Captures complex patterns automatically |\n",
    "\n",
    "### Production Advantages\n",
    "\n",
    "- **Scalability:** Precompute item embeddings → O(1) similarity lookup\n",
    "- **Cold Start:** New users can be scored immediately from genre preferences\n",
    "- **Interpretability:** Embedding space can be visualized (t-SNE, UMAP)\n",
    "- **Extensibility:** Easy to add side features (actor, director, year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**Potential improvements:**\n",
    "\n",
    "1. **Attention mechanism:** Learn feature importance weights\n",
    "2. **Triplet loss:** Directly optimize embedding distances\n",
    "3. **Multi-task learning:** Predict ratings + clicks simultaneously\n",
    "4. **Temporal dynamics:** Model how preferences evolve over time\n",
    "5. **Hybrid approach:** Combine with content-based features (plot summaries, cast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. **Covington, P., Adams, J., & Sargin, E. (2016).** \"Deep neural networks for YouTube recommendations.\" *RecSys*.\n",
    "2. **He, X., et al. (2017).** \"Neural collaborative filtering.\" *WWW*.\n",
    "3. **Koren, Y., Bell, R., & Volinsky, C. (2009).** \"Matrix factorization techniques for recommender systems.\" *Computer*, 42(8)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
